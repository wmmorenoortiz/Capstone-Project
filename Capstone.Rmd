---
title: "Indian Liver Patient Records"
author: "Wilson Moreno"
date: "1/8/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The present work wishes to predict if a patient in India is going to suffer from liver disease, by means of some indicators, for example: Age, Gender, Alkaline_Phosphography, etc. The result of a categorical variable. Different types of approaches are used such as: using all variables, through a correlation matrix and by specific variables.

# Methods/Analysis

### Step 0: require package
```{r Step_0,  warning = FALSE, message=FALSE, results='hide'}
if (!require(package)) install.packages('psych', repos = "http://cran.us.r-project.org")
if (!require(package)) install.packages('knitr', repos = "http://cran.us.r-project.org")
if (!require(package)) install.packages('ggplot2', repos = "http://cran.us.r-project.org")
library(knitr)
library(ggplot2)
library(psych)
library(caret)
```

### **Step 1:** Load the data base

```{r Step_1}
database <- read.csv("Data/indian_liver_patient.csv")
```

### **Step 2:** Exploratory Data Analysis 

#### Summay Statistics
The following table shows the descriptive statistics for all the variables in the database.

```{r Step_2.1}

round(data.frame( describeBy(database, digits= 2)),1)
# tmp <- describeBy(database,
#            group = database$Gender,
#            digits= 1)

```

#### Visualization

As can be seen in the visual analysis of the data, they were divided into 2 groups by gender, in all the graphs the rates are much higher in women than in men, and tend to follow the same distribution. Variables: Age, Albumin, Albumin_and_Globulin_Ratio and Total_Protiens, are suspected to follow a normal distribution. 

```{r Step_2.2, , message=FALSE, results='hide', warning=FALSE}
scale_database <- database %>% select(-Gender,-Dataset) %>% scale() %>% as.data.frame() %>% 
  cbind(Gender = database$Gender)

database.gathered <- scale_database  %>% as.data.frame() %>% 
  gather(key = "variable", value = "value", - Gender)

ggplot(data = database.gathered , mapping = aes(x = value, color = Gender)) +
  geom_histogram() +
  facet_wrap(facets =  vars(variable ))

```

This graph certifies the suspicion that the aforementioned variables follow a normal distribution.

```{r Step_2.2.1, , message=FALSE, results='hide', warning=FALSE}
ggplot(data = database.gathered , mapping = aes(sample = value, color = Gender)) +
  stat_qq() +  stat_qq_line(color = "black") +
  facet_wrap(facets =  vars(variable )) 

```

### **Step 3:**  Split the database in training and testing


#### Split the database in training and testing

```{r Step_3, , message=FALSE, results='hide', warning=FALSE}
set.seed(755)
test_index <- createDataPartition(y = database$Dataset, times = 1,
                                  p = 0.2, list = FALSE)

train_set <- database[-test_index,]
test_set <- database[test_index,]


RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}


```


How will the logistic regression algorithm be used **glm ()** you have to modify the response variable **Data set** to values of 0 if the person does not suffer the disease and 1 if a person suffers the disease and convert the gender variable into a factor.

```{r Step_3.1, , message=FALSE, results='hide', warning=FALSE}
# Modify the response variable.
train_set$Dataset[train_set$Dataset == 1] = 1
train_set$Dataset[train_set$Dataset == 2] = 0
test_set$Dataset[test_set$Dataset == 1] = 1
test_set$Dataset[test_set$Dataset == 2] = 0

# convert the variable into factor
train_set$Gender <- as.factor(train_set$Gender)
test_set$Gender <- as.factor(test_set$Gender)

```

### **Step  4:** Choose the Model and train the model with the training base
For the following approach some models are used, which are a function of the predictive variables to choose.

#### Model 1: Using all the variables

```{r Step_4.1, , message=FALSE , warning=FALSE}
mol_1 <- glm(Dataset ~. , data = train_set, family = binomial() )
```

#### Model 2: Variables + correlation

```{r Step_4.2.1, , message=FALSE , warning=FALSE}
round(cor(database[,-2]),2)
```
From the correlation matrix it can be observed that there is a high degree of correlation between the variables ** Total_Bilirubin ** with ** Direct_Bilirubin ** and ** Albumin ** with ** Total_Protiens **, therefore, it can be removed from the database.

```{r Step_4.2.2, , message=FALSE , warning=FALSE}
train_set_mol_2 = train_set %>% select(-Total_Bilirubin,-Total_Protiens)
mol_2 <- glm(Dataset ~. , data = train_set_mol_2, family = binomial())
```

#### Model 3: Significant variables

```{r Step_4.3.1, , message=FALSE , warning=FALSE}
summary(mol_1)
```

From the summary of model 1, where all the variables were used, it can be seen that the significant ones where their p-value is minus 0.05 are: ** Age ** and ** Alamine_Aminotransferase **. Therefore, only those variables are selected.

```{r Step_4.3.2, , message=FALSE , warning=FALSE}
train_set_mol_3 = train_set %>% select(Age,Alamine_Aminotransferase)
mol_3 <- glm(Dataset ~. , data = train_set, family = binomial())
```

### **Step  5:** Predict the possible ratings for the test base
#### Model 1: Using all the variables

```{r Step_5.1, , message=FALSE , warning=FALSE}
y_hat_1 <- predict(mol_1,test_set,type = "response")
glm.pred_1  <- ifelse(y_hat_1 > 0.5, "1", "0")
accuracy <- data_frame(method="Using all the variables",
                       Accuracy = mean(glm.pred_1 == test_set$Dataset))
```

```{r Step_5.2, , message=FALSE , warning=FALSE}
y_hat_2 <- predict(mol_2,test_set,type = "response")
glm.pred_2  <- ifelse(y_hat_2 > 0.5, "1", "0")
accuracy <- bind_rows(accuracy,data_frame(method="Variables + correlation",
                       Accuracy = mean(glm.pred_2 == test_set$Dataset)))
```

```{r Step_5.3, , message=FALSE , warning=FALSE}
y_hat_3 <- predict(mol_3,test_set,type = "response")
glm.pred_3  <- ifelse(y_hat_3 > 0.5, "1", "0")
accuracy <- bind_rows(accuracy,data_frame(method="Significant variables",
                                          Accuracy = mean(glm.pred_3 == test_set$Dataset)))
```

### **Step  6:** Accuracy
```{r Step_6, , message=FALSE , warning=FALSE}
accuracy
```

# Results
As more relevant results in the exploratory analysis it can be observed that the distribution in men and women for the different variables is similar, this allows us to think that gender is not a relevant variable to take into account, that it can be justified because it does not It is significant in the analysis of the third model. On the basis of the second graph, it can also be observed that certain variables are distributed normally, which is good, if one wishes to make univariate predictions of them. After partitioning the data in training and testing, to be later modeled by means of different approaches, it is evident that through the precision that model 1 and model 3 are equal, take into account that model 3 It only consists of 2 variables (**Age** and **Alamine_Aminotransferase**) to achieve this accuracy, therefore, they are the most important characteristics to know and predict if a patient will suffer from a liver problem.

# Conclusion

From the researched literature, decision trees or Vector Support Machine could be taken as models to achieve better prediction levels.
